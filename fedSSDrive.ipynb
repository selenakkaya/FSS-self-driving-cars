{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e871a67-8d1e-4034-a878-32c297d490d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd116771-6e83-4e44-a64e-4b573cb907b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' # 'cuda' or 'cpu'\n",
    "\n",
    "NUM_CLASSES = 19   \n",
    "\n",
    "BATCH_SIZE = 16                     \n",
    "LR = 0.05    # The initial Learning Rate\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
    "NUM_EPOCHS = 5   # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 20    # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "LOG_FREQUENCY = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dba0e2f5-2c4f-4c27-bb7d-bb3a68c7c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Cityscapes import Cityscapes\n",
    "normalizer = transforms.Normalize(mean=(0.285, 0.323, 0.282), std=(0.5, 0.5, 0.5))\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize((512, 256)), transforms.ToTensor()])\n",
    "eval_transform = transforms.Compose([transforms.Resize((512, 256)), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = Cityscapes('data/', partition_type='A', split='train', transform=train_transform)\n",
    "test_dataset = Cityscapes('data/', partition_type='A', split='val', transform=eval_transform)\n",
    "\n",
    "#i, l = train_dataset.__getitem__(1)\n",
    "#plt.imshow(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c29c65e-2fc9-43a3-a920-c110eceaf121",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58aac52-8cc2-4e95-a331-98a392db54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.7/site-packages (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "from bisenetv2 import BiSeNetV2\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "net = BiSeNetV2(3, output_aux=False, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ac874ea-1c21-4512-8290-92b24268d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss() # for classification, we use Cross Entropy\n",
    "\n",
    "parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters of AlexNet\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Define scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6dfa8e6-d1d3-4462-8419-53b5bb7c52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def mIoU(y_true, y_pred, numClass):\n",
    "    correct_pred = torch.zeros(numClass) # int = (mesh : M, wire : W and background : B)\n",
    "    den = torch.zeros(numClass) # den = M + W + B = (M or W or B) + (M and W and B) + ..\n",
    "\n",
    "    for i in range (y_true.shape[0]):\n",
    "        for j in range(y_true.shape[1]):\n",
    "            for k in range(y_true.shape[2]):\n",
    "                if (y_pred[i][j][k])==(y_true[i][j][k]):\n",
    "                    correct_pred[(y_true[i][j][k])]+=1\n",
    "                den[(y_pred[i][j][k])] += 1\n",
    "                den[(y_true[i][j][k])] += 1\n",
    "    mIoU = 0\n",
    "    IoU_classes = []\n",
    "    for i in range(numClass):\n",
    "        if den[i]!=0:\n",
    "            IoU =correct_pred[i]/(den[i]-correct_pred[i])\n",
    "            IoU_classes.append(IoU)\n",
    "\n",
    "    mIoU=sum(IoU_classes)/len(IoU_classes)\n",
    "    return mIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb5e4b93-dbd9-4812-afa5-11370663045e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5, LR = [0.05]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1795/2142107243.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Compute loss based on output and ground truth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mmIoU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmIoU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Log loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mLOG_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1795/1482022745.py\u001b[0m in \u001b[0;36mmIoU\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m                     \u001b[0mcorrect_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# By default, everything is loaded to cpu\n",
    "net = net.to(DEVICE) # this will bring the network to GPU if DEVICE is cuda\n",
    "\n",
    "#cudnn.benchmark # Calling this optimizes runtime\n",
    "\n",
    "current_step = 0\n",
    "# Start iterating over the epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('Starting epoch {}/{}, LR = {}'.format(epoch+1, NUM_EPOCHS, scheduler.get_lr()))\n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    for images, labels in train_dataloader:\n",
    "        # Bring data over the device of choice\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        net.train() # Sets module in training mode\n",
    "\n",
    "        # PyTorch, by default, accumulates gradients after each backward pass\n",
    "        # We need to manually set the gradients to zero before starting a new iteration\n",
    "        optimizer.zero_grad() # Zero-ing the gradients\n",
    "\n",
    "        # Forward pass to the network\n",
    "        outputs = net(images)\n",
    "        #print(outputs.size())\n",
    "\n",
    "        # Compute loss based on output and ground truth\n",
    "        loss = criterion(outputs, labels)\n",
    "        mIoU = mIoU(outputs, labels, NUM_CLASSES)\n",
    "        # Log loss\n",
    "        if current_step % LOG_FREQUENCY == 0:\n",
    "            print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        # Compute gradients for each layer and update weights\n",
    "        loss.backward()  # backward pass: computes gradients\n",
    "        optimizer.step() # update weights based on accumulated gradients\n",
    "        mIoU.step()\n",
    "        current_step += 1\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30e2137-c5f2-4708-baa6-73c8a6a9794f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-12.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-12:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
