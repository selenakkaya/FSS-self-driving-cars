{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YsauDE1ITL0C"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h0dyJPD1aUcr"},"outputs":[],"source":["import sys\n","sys.path.insert(0, '/content/drive/My Drive/Step2')\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchvision\n","from torchvision import transforms\n","from torchvision.transforms import InterpolationMode\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import torch.nn as nn\n","\n","from utils import display_prediction, decode_segmap\n","from Cityscapes import Cityscapes\n","from bisenetv2 import BiSeNetV2\n","from deeplabv3 import deeplabv3_mobilenetv2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZeja5CiNfv7"},"outputs":[],"source":["DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Running on {DEVICE}')\n","\n","NUM_CLASSES = 19    \n","\n","BATCH_SIZE = 8       # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n","                     # the batch size, learning rate should change by the same factor to have comparable results\n","\n","LR = 0.05            # The initial Learning Rate\n","MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n","WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n","\n","NUM_EPOCHS = 10       # Total number of training epochs (iterations over dataset)\n","STEP_SIZE = 25        # How many epochs before decreasing learning rate (if using a step-down policy)\n","GAMMA = 0.1           # Multiplicative factor for learning rate step-down\n","\n","LOG_FREQUENCY = 40\n","\n","RESIZED_H = 512\n","RESIZED_W = 1024\n","\n","MODE = 'TRAIN'       #TRAIN or LOAD_AND_TRAIN or LOAD\n","MODEL_PATH = '/content/drive/My Drive/Step2/mobilenet.tar' #File to save the model in"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WymM0IXwdCfQ"},"outputs":[],"source":["mean = [0.3257, 0.3690, 0.3223]\n","std = [0.2112, 0.2148, 0.2115]\n","normalizer = transforms.Normalize(mean=mean, std=std)\n","resizer_bicubic = transforms.Resize((RESIZED_H, RESIZED_W), interpolation=InterpolationMode.BICUBIC)\n","resizer_nearest = transforms.Resize((RESIZED_H, RESIZED_W), interpolation=InterpolationMode.NEAREST)\n","\n","train_transform = transforms.Compose([resizer_bicubic])\n","eval_transform = transforms.Compose([resizer_bicubic])\n","target_transform = transforms.Compose([resizer_nearest])\n","\n","train_dataset = Cityscapes('drive/MyDrive/', partition_type='A', split='train', transform=train_transform, target_transform=target_transform)\n","test_dataset = Cityscapes('drive/MyDrive/', partition_type='A', split='val', transform=eval_transform, target_transform=target_transform)\n","\n","i, l = train_dataset.__getitem__(101)\n","fig = plt.figure(figsize=(25, 15))\n","fig.add_subplot(1, 2, 1)\n","plt.imshow(i.permute(1, 2, 0))\n","fig.add_subplot(1, 2, 2)\n","l = decode_segmap(l.squeeze())\n","plt.imshow(l)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wv3RjSsWCkBM"},"outputs":[],"source":["len(train_dataset)\n","len(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKORVDmN6pr1"},"outputs":[],"source":["train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","val_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifO3y-DFTBg2"},"outputs":[],"source":["#net = BiSeNetV2(NUM_CLASSES, output_aux=False, pretrained=False)\n","net = deeplabv3_mobilenetv2(num_classes=19, in_channels=3)\n","net = net.to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqa9DA_MPgvF"},"outputs":[],"source":["# Define loss function\n","criterion = nn.CrossEntropyLoss(ignore_index=255) # for classification, we use Cross Entropy\n","\n","# Choose parameters to optimize\n","# To access a different set of parameters, you have to access submodules of the net\n","# (nn.Module objects implement the Composite Pattern)\n","# e.g.: parameters of the fully connected layers: net.classifier.parameters()\n","# e.g.: parameters of the convolutional layers\n","parameters_to_optimize = net.parameters() # In this case we optimize over all the parameters\n","\n","# Define optimizer\n","optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","\n","# Define scheduler\n","# A scheduler dynamically changes learning rate\n","# The most common schedule is the step(-down), which multiplies learning rate by gamma every STEP_SIZE epochs\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPXgbBItM-fd"},"outputs":[],"source":["!pip install torchmetrics\n","import torchmetrics\n","from torchmetrics.classification import MulticlassJaccardIndex\n","metric = MulticlassJaccardIndex(num_classes=NUM_CLASSES, ignore_index=255).to(DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5OclhYEuVXHG"},"outputs":[],"source":["def train_epoch(net, dataloader, lr=0.01, optimizer=optimizer, loss_fn=criterion):\n","  net.train()\n","\n","  total_loss, miou, count = 0, 0, 0\n","  for i, (images, masks) in enumerate(dataloader):\n","    images = images.to(DEVICE, dtype=torch.float32)\n","    masks = masks.squeeze().to(DEVICE, dtype=torch.long)\n","    optimizer.zero_grad()\n","\n","    out = net(images)['out']\n","    loss = loss_fn(out, masks) \n","    loss.backward()\n","    optimizer.step()\n","\n","    total_loss += loss\n","    miou += metric(out, masks)\n","    count += 1\n","    if i%LOG_FREQUENCY == 0:\n","      print(\"  minibatch {}: train mIoU = {} train loss = {}\".format(i, miou.item()/count, total_loss.item()/count))\n","  return total_loss.item()/count, miou.item()/count\n","\n","def validate(net, dataloader, loss_fn=criterion):\n","  net.eval()\n","  loss, miou, count = 0, 0, 0\n","  with torch.no_grad():\n","    for images, masks in dataloader:\n","      images = images.to(DEVICE)\n","      masks = masks.squeeze().to(DEVICE)\n","\n","      out = net(images)['out']\n","      loss += loss_fn(out, masks) \n","      miou += metric(out, masks)\n","      count += 1\n","  return loss.item()/count, miou.item()/count\n","\n","def train(net, train_loader, test_loader, history=None, starting_epoch=0, optimizer=optimizer, lr=0.01, epochs=10, loss_fn=criterion):\n","  history = history or {'train_loss' : [0.0], 'train_miou': [0.0], 'val_loss': [0.0], 'val_miou': [0.0]} \n","\n","  print(f'Lr: {lr}\\nBatch size: {BATCH_SIZE}\\nEpochs: {epochs}\\nLoss function: {loss_fn}\\nOptimizer: {optimizer}')\n","\n","  for ep in range(starting_epoch, starting_epoch+epochs):\n","    print(f'----------- EPOCH {ep+1} -----------')\n","    train_loss, train_miou = train_epoch(net, train_loader, optimizer=optimizer, lr=lr, loss_fn=loss_fn)\n","    val_loss, val_miou = validate(net, test_loader, loss_fn=loss_fn)\n","    print(f\"Train mIoU={train_miou:.3f}, Val mIoU={val_miou:.3f}, Train loss={train_loss:.3f}, Val loss={val_loss:.3f}\")\n","\n","    save = False\n","    if val_miou \u003e max(history['val_miou']): #If this is the best validation mIoU, save the model\n","      save = True\n","      print(f'** Saving model with mIoU = {val_miou}')\n","\n","    history['train_loss'].append(train_loss)\n","    history['train_miou'].append(train_miou)\n","    history['val_loss'].append(val_loss)\n","    history['val_miou'].append(val_miou)\n","    display_prediction(net, test_dataset, random=False)\n","    scheduler.step()\n","\n","    if save == True:\n","      torch.save({\n","            'model_state_dict': net.state_dict(),\n","            'optim_state_dict': optimizer.state_dict(),\n","            'epoch': ep,\n","            'history': history,\n","            'batch_size': BATCH_SIZE,\n","            'lr': lr,\n","            'resized_height': RESIZED_H,\n","            'resized_width': RESIZED_W,\n","        }, MODEL_PATH)\n","\n","  return history\n","\n","def plot_history(history):\n","  plt.figure(figsize=(15,5))\n","  plt.subplot(121)\n","  plt.plot(history['train_miou'][1:], label='Training mIoU')\n","  plt.plot(history['val_miou'][1:], label='Validation mIoU')\n","  plt.legend()\n","  plt.subplot(122)\n","  plt.plot(history['train_loss'][1:], label='Training loss')\n","  plt.plot(history['val_loss'][1:], label='Validation loss')\n","  plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"2ElxCIX5eWYR"},"outputs":[{"name":"stdout","output_type":"stream","text":["Lr: 0.05\n","Batch size: 8\n","Epochs: 10\n","Loss function: CrossEntropyLoss()\n","Optimizer: SGD (\n","Parameter Group 0\n","    dampening: 0\n","    differentiable: False\n","    foreach: None\n","    initial_lr: 0.05\n","    lr: 0.05\n","    maximize: False\n","    momentum: 0.9\n","    nesterov: False\n","    weight_decay: 5e-05\n",")\n","----------- EPOCH 1 -----------\n","  minibatch 0: train mIoU = 0.010452542454004288 train loss = 2.9915108680725098\n","  minibatch 40: train mIoU = 0.17653404794088223 train loss = 0.9094495075504955\n"]}],"source":["def load_model(path):\n","  checkpoint = torch.load(MODEL_PATH)\n","  net.load_state_dict(checkpoint['model_state_dict'])\n","  #optimizer.load_state_dict(checkpoint['optim_state_dict'])\n","  epoch = checkpoint['epoch'] + 1\n","  history = checkpoint['history']\n","  mIoU = history['val_miou'][-1]\n","  print(f'Loading pre-trained model at epoch {epoch}, mIoU={mIoU}')\n","  plot_history(history)\n","  return epoch, history\n","\n","if MODE == 'LOAD':\n","  epoch, history = load_model(MODEL_PATH)\n","  net.eval()\n","elif MODE == 'LOAD_AND_TRAIN':\n","  epoch, history = load_model(MODEL_PATH)\n","  print(f'Loaded pre-trained model at epoch {epoch}')\n","  history = train(net, train_dataloader, val_dataloader, history, starting_epoch=epoch, optimizer=optimizer, lr=LR, epochs=NUM_EPOCHS)\n","  plot_history(history)\n","elif MODE == 'TRAIN':\n","  history = train(net, train_dataloader, val_dataloader, history=None, starting_epoch=0, optimizer=optimizer, lr=LR, epochs=NUM_EPOCHS)\n","  plot_history(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PsmuzPpBqgu"},"outputs":[],"source":["display_prediction(net, test_dataset, random=True)"]}],"metadata":{"colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}